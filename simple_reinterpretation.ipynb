{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pyhf\n",
    "import cabinetry\n",
    "import redist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinterpetation with **redist**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"fig/logo.svg\"  width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "**redist** performs histogram reweighting based on differences in kinematic distributions. It is built to operate in combination with [pyhf](https://pyhf.readthedocs.io).\n",
    "\n",
    "**Check out the paper [here](https://arxiv.org/pdf/2402.08417.pdf).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple analysis with [`pyhf`](https://pyhf.readthedocs.io/en/v0.7.6/intro.html)\n",
    "\n",
    "[`pyhf`](https://pyhf.readthedocs.io/en/v0.7.6/intro.html) ( = HistFactory in python) has become a common analysis tool for statistical inference.\n",
    "\n",
    "It is based on maximum likelihood estimation with the (very simplified) likelihood function for observed event counts $\\boldsymbol{n}$\n",
    "\n",
    "$$\n",
    "    L(\\boldsymbol{n}, \\boldsymbol{a} \\mid \\boldsymbol{\\eta}, \\boldsymbol{\\chi})=\\underbrace{\\operatorname{Pois}\\left(\n",
    "    \\boldsymbol{n} \\mid \n",
    "    \\boldsymbol{\\nu}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\right)}_{\\text{data likelihood}}\n",
    "    \\underbrace{c\\left(\\boldsymbol{a} \\mid \\boldsymbol{\\chi} \\right)}_{\\text{constraint likelihood}}.\n",
    "$$\n",
    "\n",
    "The expected number of events are\n",
    "\n",
    "$$  \n",
    "    \\boldsymbol{\\nu}\\left(\\boldsymbol{\\eta}, \\boldsymbol{\\chi}\\right)\n",
    "    =\n",
    "    \\boldsymbol{\\kappa}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\left(\n",
    "    \\boldsymbol{\\nu}^0(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})+\\boldsymbol{\\Delta}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\right) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The $A \\to B C D$ decay\n",
    "\n",
    "We want to measure the decay $A \\to B C D$, which has a single kinematic degree of freedom $z$. \n",
    "\n",
    "### Simulate some data\n",
    "\n",
    "We expect the **signal** to be normally distributed according to the PDF\n",
    "$$\n",
    "p(z) = \\mathcal{N}(z|\\mu=4, \\sigma=1.5)\n",
    "$$\n",
    "and our **background** to be distributed according to the PDF\n",
    "$$\n",
    "p(z) = \\frac{1}{5}e^{-z/5}.\n",
    "$$\n",
    "\n",
    "We expect 10 times more background than signal, so we simulate\n",
    "\n",
    "- 10k signal events,\n",
    "- 100k background events.\n",
    "\n",
    "We also need to simulate our detector. It accepts 10% of events at random and smears our kinematic variable $z$ normally, with $\\sigma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 4.\n",
    "\n",
    "sig_samples_gen = sp.stats.norm.rvs( loc=mean, scale=1.5, size= 10000)\n",
    "bkg_samples_gen = sp.stats.expon.rvs(loc=0, scale=5., size=100000)\n",
    "\n",
    "def detector(z):\n",
    "    acceptance = np.random.rand(*np.shape(z)) > 0.9\n",
    "    resolution = sp.stats.norm.rvs(z)\n",
    "    return resolution[acceptance], z[acceptance]\n",
    "\n",
    "sig_samples_detector, sig_samples = detector(sig_samples_gen)\n",
    "bkg_samples_detector, bkg_samples = detector(bkg_samples_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide that the reconstructed kinematic variable, after considering detector effects $x \\sim \\varepsilon(x|z)$ distinguishes signal and background distributions well for $0<x<10$ and chose it as a fitting variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,10)\n",
    "\n",
    "# We only want to keep the events with physical values for z\n",
    "sig_samples_detector[(sig_samples < bins[0]) | (sig_samples > bins[-1])] = -1\n",
    "bkg_samples_detector[(bkg_samples < bins[0]) | (bkg_samples > bins[-1])] = -1\n",
    "\n",
    "sig_hist, _ = np.histogram(sig_samples_detector, bins=bins)\n",
    "bkg_hist, _ = np.histogram(bkg_samples_detector, bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our detector changed the shape of our distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sig_samples, bins=bins, histtype='step', color='g', linestyle='dashed',label='true sig')\n",
    "plt.hist(bkg_samples, bins=bins, histtype='step', color='b', linestyle='dashed',label='true bkg')\n",
    "\n",
    "plt.stairs(sig_hist, color='g', label='measured sig')\n",
    "plt.stairs(bkg_hist, color='b', label='measured bkg')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data from our detector\n",
    "\n",
    "We are now ready to look at some \"real\" data. \n",
    "\n",
    "We find that in reality, our signal is actually distributed according to the PDF\n",
    "$$\n",
    "p(z) = \\mathcal{N}(z|\\mu=5, \\sigma=1.5).\n",
    "$$\n",
    "In addition, there are actually twice as many signal events, as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_true = 5.\n",
    "scale_true = 2.\n",
    "\n",
    "bkg_samples_data = sp.stats.expon.rvs(scale=5., size=100000)\n",
    "sig_samples_data = sp.stats.norm.rvs(loc=mean_true, scale=1.5, size= 10000)\n",
    "\n",
    "bkg_samples_data, _ = detector(bkg_samples_data)\n",
    "sig_samples_data, _ = detector(sig_samples_data)\n",
    "\n",
    "bkg_hist_data, _ = np.histogram(bkg_samples_data, bins=bins)\n",
    "sig_hist_data, _ = np.histogram(sig_samples_data, bins=bins)\n",
    "\n",
    "data = (bkg_hist_data + sig_hist_data * scale_true).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "We construct our statistical model and add a Poisson error for our background. We use `cabinetry` for visualization here. it also calculates the correct uncertainties for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyhf.simplemodels.uncorrelated_background(\n",
    "    signal=sig_hist.tolist(), \n",
    "    bkg=bkg_hist.tolist(), \n",
    "    bkg_uncertainty=np.sqrt(bkg_hist).tolist()\n",
    ")\n",
    "\n",
    "model_pred_prefit = cabinetry.model_utils.prediction(model)\n",
    "cabinetry.visualize.data_mc(model_pred_prefit, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not look too good yet.\n",
    "\n",
    "Maybe after fitting, things will look better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results = cabinetry.fit.fit(model, data + model.config.auxdata)\n",
    "for label, result, unc in zip(fit_results.labels, fit_results.bestfit, fit_results.uncertainty):\n",
    "    print(f\"{label}: {result:.3f} +/- {unc:.3f}\")\n",
    "    \n",
    "model_pred_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "cabinetry.visualize.data_mc(model_pred_postfit, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we got the scale factor correct! But would you really trust a best fit result that looks like this?\n",
    "\n",
    "To make things better, we decide that the signal PDF might not be the correct one. It would be great if we could leave the mean free floating here and also learn something about this parameter:\n",
    "$$\n",
    "p(z|m) = \\mathcal{N}(z|\\mu=m, \\sigma=1.5).\n",
    "$$\n",
    "\n",
    "In reality there is no easy way to do this. You would have to\n",
    "1. Pick an alternative value for $m$,\n",
    "2. Generate new MC samples,\n",
    "3. Simulate the detector response to each of the samples,\n",
    "4. Redo the statistical analysis,\n",
    "\n",
    "and repeat this until you found a good value for $m$. \n",
    "\n",
    "Phu...\n",
    "\n",
    "What about a better way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The [`redist`](https://github.com/lorenzennio/redist) reinterpretation method\n",
    "\n",
    "After reconstruction you obtain a number-density $n(x)$ for all channels, in a reconstruction/fitting variable $x$. Stacking all signal and background contributions, this is what we compare to the data.\n",
    "\n",
    "When producing MC samples, we made a choice - the theoretical prediction \n",
    "$$\n",
    "    \\sigma(z) = \\sigma p(z)\n",
    "$$\n",
    "in the kinematic variable(s) $z$. Usually, this is a SM prediction, but can be any theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"fig/redist.drawio.svg\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "The number density $n(x)$ and the theoretical prediction $\\sigma(z)$ are connected via\n",
    "$$\n",
    "    n(x) = \\sum_z ~ L ~ \\varepsilon(x|z) ~ \\sigma(z)\n",
    "$$\n",
    "where we fold the theoretical prediction with the conditional reconstruction and selection efficiency $\\varepsilon(x|z)$ and scale by the corresponding luminosity $L$.\n",
    "\n",
    "Futher, we can define the **joint number density** \n",
    "$$\n",
    "n(x,z) = L ~ \\varepsilon(x|z) ~ \\sigma(z).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"fig/redist-rw.drawio.svg\"  width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "With this object alone, we can obtain the bin yields $n_B(x)$ of an alternative prediction $\\sigma_B(z)$,\n",
    "$$\n",
    "    n_B(x)\n",
    "        = \\sum_z ~ L ~ \\varepsilon(x|z) ~ \\sigma_B(z)\n",
    "        = \\sum_z ~ L ~ \\varepsilon(x|z) ~ \\sigma_A(z) ~ \\frac{\\sigma_B(z)}{\\sigma_A(z)}\n",
    "        = \\sum_z ~ n_A(x,z) ~ w(z).\n",
    "$$\n",
    "Here, the weights $w(z)$ are the ratio of the bin integrated kinematic predictions. \n",
    "\n",
    "Analysts can easily compute the joint number density $n_A(x,z)$ for their underlying theory with a kinematic prediction $\\sigma_A(z)$ from the MC samples.\n",
    "\n",
    "Let us apply this to our example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "Recall how [`pyhf`](https://pyhf.readthedocs.io/en/v0.7.6/intro.html) calculates the expected number of events,\n",
    "\n",
    "$$  \n",
    "    \\boldsymbol{\\nu}\\left(\\boldsymbol{\\eta}, \\boldsymbol{\\chi}\\right)\n",
    "    =\n",
    "    \\boldsymbol{\\kappa}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\left(\n",
    "    \\boldsymbol{\\nu}^0(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})+\\boldsymbol{\\Delta}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\right) .\n",
    "$$\n",
    "\n",
    "We introduce a new bin-wise, multiplicative modifier \n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\kappa}(x | \\boldsymbol{\\eta}, \\boldsymbol{\\chi}) = \\frac{n_B(x|\\boldsymbol{\\eta}, \\boldsymbol{\\chi})}{n_A (x|\\boldsymbol{\\eta}, \\boldsymbol{\\chi})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null and alternative kinematics\n",
    "\n",
    "First, let us define the theory predictions of our null hypothesis,\n",
    "$$\n",
    "    \\sigma_{A}(z) = \\mathcal{N}(z|\\mu=4, \\sigma=1.5).\n",
    "$$\n",
    "\n",
    "Next we define the kinematic prediction for our alternative hypothesis:\n",
    "- We want a free floating $m$ parameter. \n",
    "- In addition, based on your previous knowledge, we incorporate the scaling factor of 2 into the new model as well\n",
    "$$\n",
    "\\sigma_{B}(z|m) = 2 \\cdot \\mathcal{N}(z|\\mu=m, \\sigma=1.5).\n",
    "$$\n",
    "\n",
    "(Only the relative scaling matters here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_A(x):\n",
    "    return sp.stats.norm.pdf(x, loc=4., scale=1.5)\n",
    "\n",
    "def sigma_B(x, mu):\n",
    "    scale = 2.\n",
    "    return scale*sp.stats.norm.pdf(x, loc=mu, scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the joint number densities\n",
    "\n",
    "Creating the joint number density object is easy - it is just a 2d histogram.\n",
    "\n",
    "We can make use of the `redist.modifier.map()` function to place our samples in the correct bins.\n",
    "\n",
    "For simplicity let us use the same binning for our kinematic degree of freedom $z$, as we used for our reconstruction variable $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_nr_dens = redist.modifier.map(\n",
    "    sig_samples_detector, \n",
    "    [sig_samples], \n",
    "    bins, \n",
    "    [bins]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the custom modifier\n",
    "\n",
    "Next we can define the custom modifier that will reweight our histograms internally, initialized with the `redist.modifier.Modifier()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we only need to define our new parameter $m$ in a dictionary. Your theory friend tells you $m=3.5$ is a good choice (everyone can be wrong sometimes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = {\n",
    "    'm' :{'inits': (3.5,), 'bounds': ((0., 10.),), 'paramset_type': 'unconstrained'},\n",
    "    }\n",
    "\n",
    "cmod = redist.modifier.Modifier(\n",
    "    new_params, \n",
    "    sigma_B, \n",
    "    sigma_A, \n",
    "    joint_nr_dens, \n",
    "    [bins], \n",
    "    name=\"vary_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We can visualize the kinematic predictions, including weights with `redist.plot.dists()` and the joint number density with `redist.plot.map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [3.5]\n",
    "redist.plot.dists(cmod, m, plot_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redist.plot.map(cmod, cmap='YlOrRd', aspect='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to the statistical model\n",
    "\n",
    "To add this to our existing `pyhf.Model`, we can use the helper function `redist.add_to_model()`. \n",
    "\n",
    "Let us have a look at the model parameters now afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mod = {\n",
    "                \"name\": \"signal_vary_m\",\n",
    "                \"type\": \"vary_m\", # the name of the custom modifier, provided above \n",
    "                \"data\":\n",
    "                    {\n",
    "                        \"expr\": \"vary_m_weight_fn\", # the name of the custom function assigned in global namespace = modifier.name + '_weight_fn'\n",
    "                    }\n",
    "              }\n",
    "\n",
    "model = redist.modifier.add_to_model(model, ['singlechannel'], ['signal'], cmod.expanded_pyhf, custom_mod)\n",
    "\n",
    "model.config.par_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the parameters\n",
    "\n",
    "We can fit directly for $m$ now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results= cabinetry.fit.fit(model, data + model.config.auxdata)\n",
    "for label, result, unc in zip(fit_results.labels, fit_results.bestfit, fit_results.uncertainty):\n",
    "    print(f\"{label}: {result:.3f} +/- {unc:.3f}\")\n",
    "\n",
    "model_pred_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "_ = cabinetry.visualize.data_mc(model_pred_postfit, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we managed to actually find the correct value for $m$ AND $\\mu$ -- and a much better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not shown today\n",
    "\n",
    "- Save to `json`: `redist.modifier.save()` \n",
    "  \n",
    "- Load from `json`: `redist.modifier.load()`\n",
    "\n",
    "- Correlated parameters\n",
    "  ```\n",
    "  new_params = {'f' :{'inits': ..., 'bounds': ..., 'cov': ..., 'paramset_type': ...}}\n",
    "  ```\n",
    "\n",
    "- Combination of models: `redist.modifier.combine()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- [`redist`](https://github.com/lorenzennio/redist) is an **implementation** of the presented reinterpretation method.\n",
    "  \n",
    "- It is built to work with [`pyhf`](https://pyhf.readthedocs.io/en/v0.7.6/intro.html).\n",
    "\n",
    "- It allows for **fast** reinterpretation with minimal additional information.\n",
    "\n",
    "- This enables **bias free** inference of shape changing parameters.\n",
    "\n",
    "- The method is more widely applicable. It would be great to see this work with other tools. Should be easy with e.g. [zfit](https://zfit.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Posterior estimation with [`bayesian_pyhf`](https://github.com/malin-horstmann/bayesian_pyhf)\n",
    "\n",
    "One can construct a posterior model simply from [`pyhf`](https://pyhf.readthedocs.io/en/v0.7.6/intro.html) likelihood:\n",
    "$$\n",
    "    p\\left( \\boldsymbol{\\eta}, \\boldsymbol{\\chi} \\vert \\boldsymbol{n}, \\boldsymbol{a} \\right) \\propto \n",
    "    \\underbrace{\\operatorname{Pois}\\left(\n",
    "    \\boldsymbol{n} \\mid \n",
    "    \\boldsymbol{\\nu}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\right)}_{\\text{data likelihood}} \n",
    "    \\underbrace{p\\left( \\boldsymbol{\\chi} | \\boldsymbol{a} \\right)}_{\\text{constraint prior}}\n",
    "    \\underbrace{p\\left( \\boldsymbol{\\eta} \\right)}_{\\text{unconstraint prior}}.\n",
    "$$\n",
    "\n",
    "With [`bayesian_pyhf`](https://github.com/malin-horstmann/bayesian_pyhf) we can sample from this posterior and obtain an estimate for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_pyhf import infer\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes some time to run - you can just load the results below\n",
    "\n",
    "# unconstr_priors = {\n",
    "#     'mu': {'type': 'Uniform_Unconstrained', 'lower': [0.], 'upper': [10.]},\n",
    "#     'm' : {'type': 'Uniform_Unconstrained', 'lower': [0.], 'upper': [10.]}\n",
    "# }\n",
    "\n",
    "# n_draws = 5000\n",
    "\n",
    "# with infer.model(model, unconstr_priors, data):\n",
    "#     post_data = pm.sample(draws=n_draws, chains=4, cores=4)\n",
    "\n",
    "# post_data.to_json( f'post_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = az.from_json('post_data.json')\n",
    "corner.corner(\n",
    "    post_data.posterior, \n",
    "    var_names=['mu', 'm'], \n",
    "    smooth=1, \n",
    "    color='r', \n",
    "    plot_datapoints=False\n",
    "    );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
